# PPO Training Configuration
# Adapted from qwop-wr templates for native Python environment
# On-policy policy gradient alternative to value-based QRDQN

# Algorithm and training duration
algorithm: PPO
total_timesteps: 5_000_000
n_envs: 4                    # Parallel environments (can increase on more cores)
max_episode_steps: 1000      # Max steps per episode before truncation

# Environment configuration
env_kwargs:
  frames_per_step: 4          # 4 physics ticks per action = 0.16s game-time
  reduced_action_set: true    # Use 9 actions instead of 16
  failure_cost: 10.0          # Penalty for falling
  success_reward: 50.0        # Bonus for completing course
  time_cost_mult: 10.0        # Time cost multiplier in reward

# PPO hyperparameters (from qwop-wr templates)
learner_kwargs:
  device: "auto"              # cuda > mps > cpu
  use_sde: false
  sde_sample_freq: 4
  n_steps: 2048               # Steps per env before update
  batch_size: 32              # Minibatch size for PPO
  n_epochs: 10                # PPO epochs per rollout
  gamma: 0.9                  # Discount factor
  gae_lambda: 0.98            # GAE lambda
  clip_range: 0.4             # PPO clip range (from qwop-wr)
  normalize_advantage: true
  ent_coef: 0.001             # Entropy coefficient (from qwop-wr)
  vf_coef: 0.5                # Value function coefficient
  max_grad_norm: 3            # Max gradient norm (from qwop-wr)
  learning_rate: 0.001
  policy_kwargs:
    net_arch: [256, 256]      # Hidden layer sizes (match QRDQN)

# Checkpointing
save_freq: 100_000            # Save checkpoint every N timesteps
checkpoint_dir: data/checkpoints
model_dir: data/models

# Logging
log_dir: data/logs
tensorboard: true
user_metrics_log_interval: 1
