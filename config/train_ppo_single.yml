# PPO Training Configuration - Single Process
# Use this config for testing or on systems with multiprocessing issues
# Slower than parallel training but more stable

# Algorithm and training duration
algorithm: PPO
total_timesteps: 1_000_000  # Reduced for testing
n_envs: 1                   # Single environment (no multiprocessing)
max_episode_steps: 1000

# Environment configuration
env_kwargs:
  frames_per_step: 4
  reduced_action_set: true
  failure_cost: 10.0
  success_reward: 50.0
  time_cost_mult: 10.0

# PPO hyperparameters
learner_kwargs:
  use_sde: false
  sde_sample_freq: 4
  n_steps: 2048
  batch_size: 32
  n_epochs: 10
  gamma: 0.9
  gae_lambda: 0.98
  clip_range: 0.4
  normalize_advantage: true
  ent_coef: 0.001
  vf_coef: 0.5
  max_grad_norm: 3
  learning_rate: 0.001
  policy_kwargs:
    net_arch: [256, 256]

# Checkpointing
save_freq: 50_000
checkpoint_dir: data/checkpoints
model_dir: data/models

# Logging
log_dir: data/logs
tensorboard: true
user_metrics_log_interval: 1
