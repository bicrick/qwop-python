# DQNfD Stage 2: Self-Play Refinement
# Brought over from qwop-wr/config
#
# Removes expert demos, refines through pure self-play.
# Load Stage 1 model: --resume-from data/DQNfD-Stage1-<run_id>/model.zip
#
# Expected: ~15M steps. Goal: consolidate techniques without expert mixing.

algorithm: QRDQN
total_timesteps: 15_000_000
n_envs: 1
max_episode_steps: 1000

# model_load_file: "data/DQNfD-Stage1-vzqtwzx4/model.zip"

env_kwargs:
  frames_per_step: 4
  reduced_action_set: true
  failure_cost: 10.0
  success_reward: 50.0
  time_cost_mult: 10.0

learner_kwargs:
  device: "auto"
  buffer_size: 300000
  learning_starts: 5000
  batch_size: 64
  tau: 1.0
  gamma: 0.997
  train_freq: 4
  gradient_steps: 1
  target_update_interval: 512
  exploration_fraction: 0.3
  exploration_initial_eps: 0.05
  exploration_final_eps: 0.001
  learning_rate: 0.0001
  policy_kwargs:
    net_arch: [256, 128]

save_freq: 250_000
checkpoint_dir: data/checkpoints
model_dir: data/models

log_dir: data/logs
tensorboard: true
user_metrics_log_interval: 1
